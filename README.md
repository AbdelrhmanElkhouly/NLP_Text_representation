# NLP_Text_representation
In Natural Language Processing (NLP) the conversion of raw-text to numerical form is called
Text Representation and believe me this step is one of the most important steps in the NLP pipeline as if we feed in poor features in ML Model,we will get poor results. 
In computer science, this is often called “garbage in, garbage out.”  I observed in NLP feeding a good text representation to an ordinary algorithm will get you much farther compared to applying a topnotch algorithm to an ordinary text representation. 
In this notebook, I will discuss various text-representation schemes with their advantages and disadvantages so that you can choose one of the schemes which suit your task most.
Our main objective is to transform a given text into numerical form so that it can be fed into NLP and ML algorithms.
we first done basic pre-processing.and then we have used some text representaion techniques and explain its advantage and disadvantages  :
1- One-Hot Encoding 
2-Bag of words 
3- Bag of N-grams 
4-TF-IFDF
5-Word2vec word Embeddings made by google --> a-CBOW(contionous bag of words) ,b-skipGram
6-Glove Word Embeddings made by stanford
7-FastText Word Embeddings made by Facebook 

